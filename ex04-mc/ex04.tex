\documentclass[a4paper]{article}
%\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{geometry}
\geometry{ left=2cm, right=2cm, top=2cm, bottom=3cm, bindingoffset=5mm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{amstext}
\usepackage{array}
\usepackage{amsmath}
\newcolumntype{L}{>{$}l<{$}}
\usepackage{tabularx, ragged2e}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{lastpage}
\usepackage{todonotes}
\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries}
\usepackage{listings}
\usepackage{color}

\usepackage{tikz}
%\newcommand{\tikzmark}[2]{\tikz[overlay, remember picture] \node[inner sep=0pt, outer sep=0pt, anchor=base] (#1) {#2};}
\usetikzlibrary{tikzmark}


\definecolor{mygreen}{rgb}{0.18, 0.545, 0.341}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{myblue}{rgb}{0.53,0.61,0.85}

\lstset{
 keywordstyle=\color{mygreen},
 commentstyle=\color{mygray},
 numbers=left,
 numbersep=5pt, 
 numberstyle=\scriptsize\color{mygray}
 }

\usepackage{amsmath,amssymb}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}% expected value

\date{}
\author{}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{Felix Burk\\ Pascal HuszÃ¡r}
\fancyhead[L]{Reinforcment Learning \\ Summer Term 2021 }
\fancyfoot[R]{page \thepage \text{ }/ \pageref*{LastPage}}
%\fancyfoot[LE]{Seite \thepage \text{ }von \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.5pt}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{\textbf{Exercise 04}}

\begin{document}
	\maketitle 
	\thispagestyle{fancy}
	
    \section*{Task 1 - Monte Carlo Methods vs Dynamic Programming}
    \begin{itemize}
    	\item[a)] \textit{What are advantages of Monte Carlo methods over dynamic programming? Mention at least two.}\\
    	
    	Example Blackjack: MC evaluates or take all the way in one episode to the terminal node. The estimates for each state are also independent to all the other states. Whereas DP includes only one-step transition and the estimates for a state build upon other states.\\
    	Another advantage is the ability of learning from a stream of experience. By playing a certain strategy for a certain state to some terminal state, valuable experiences are gathered. 
    	Also the MC method doesn't need a full knowledge of the environment as it works on sampled action-state pairs in episodes.
    	
    	\item[b)]\textit{Give an example environment where you would use a Monte Carlo method to learn the value function rather than
    		using dynamic programming. Explain why.}\\
    	
    	In a video game scenario where all the possible state exceed an observable number (e.g. AoE, League of Legends, Dota2). By sampling/generating many episodes, experiences are collected and the value function can be learned through that experience.
    \end{itemize}

\end{document}